# 2. System Architecture

The architecture of Echo is designed around a continuous, cyclical multi-agent system.

![Multi-Agent Architecture](https://i.imgur.com/your_new_multi_agent_diagram.png) *Conceptual Diagram: Replace with actual diagram*

## 2.1. The Multi-Agent Orchestration
Echo operates as an orchestrator of several specialized internal agents, each responsible for a distinct part of the Perceive-Reason-Act cycle:

1.  **`PerceptionAgent`:** This agent is the primary interface with the physical world. It takes raw sensory input (from `VisionTool` and `AudioTool`) and processes it into structured, high-level observations, mimicking the capabilities of a Visual Language Model (VLM). It provides rich scene descriptions and identified objects.
2.  **`MemoryStream`:** This is the central, on-device knowledge base. It stores the structured observations and events generated by the `PerceptionAgent` as memories, enabling long-term context and recall.
3.  **`DecisionAgent`:** This is the LLM-powered brain of Echo. It receives the structured observations from the `PerceptionAgent` and queries the `MemoryStream` for relevant past experiences. It then uses the on-device LLM to analyze this combined context and decide if a helpful suggestion or a direct action is needed.
4.  **`ActionAgent`:** This agent is responsible for executing commands issued by the `DecisionAgent`. It interfaces with various device functionalities or other applications (e.g., `MusicPlayer`).

## 2.2. The Enhanced Perceive-Reason-Act Cycle
-   **Perceive (by `PerceptionAgent`):** Raw sensor data is transformed into rich, VLM-like scene descriptions and object lists.
-   **Store (by Orchestrator to `MemoryStream`):** Relevant observations are added to the `MemoryStream`.
-   **Reason (by `DecisionAgent`):** The LLM analyzes the current observations and relevant memories to determine the optimal response or action.
-   **Act (by `ActionAgent`):** The `ActionAgent` executes commands from the `DecisionAgent` or the orchestrator delivers a subtle suggestion.

## 2.3. Component Breakdown
-   **Sensory Tools (`VisionTool`, `AudioTool`):** Low-level interfaces to hardware. The `VisionTool` now provides VLM-like output (scene descriptions, object lists).
-   **`MusicPlayer`:** An example of an external application or device Echo can control.
-   **`MemoryStream`:** An on-device vector database for semantic memory storage and retrieval.
-   **`PerceptionAgent`:** Orchestrates sensory input, performs initial processing (e.g., VLM inference), and structures observations.
-   **`DecisionAgent`:** The LLM-powered core that performs high-level reasoning and decision-making.
-   **`ActionAgent`:** Executes specific commands on behalf of the `DecisionAgent`.